{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 5: Random Forests\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement a **RANDOM FOREST**.\n",
    "\n",
    "Complete all the **required code sections**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. Companies are naturally interested in churn, i.e., in which users are likely to switch to another company soon to get a better deal, and which are more loyal customers.\n",
    "\n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y\n",
    "\n",
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the train set: 2817\n",
      "Number of samples in the test set: 940\n",
      "Number of churned users in test: 479\n",
      "Number of loyal users in test: 461\n",
      "Mean of the training input data: [-0.  0. -0.]\n",
      "Std of the training input data: [1. 1. 1.]\n",
      "Mean of the test input data: [0.0575483  0.05550169 0.0073833 ]\n",
      "Std of the test input data: [0.98593187 0.97629659 1.00427583]\n",
      "[-1  1  1 ...  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Compute the splits\n",
    "m_training = int(0.75*X.shape[0])\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training]\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"Number of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# Standardize the input matrix\n",
    "# The transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))\n",
    "print(Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **homogeneous coordinates** to describe all the coefficients of the model.\n",
    "\n",
    "_Hint:_ The conversion can be performed with the function $hstack$ in $numpy$.\n",
    "\n",
    "\n",
    "**^**\n",
    "\n",
    "**l**\n",
    "\n",
    "**l**\n",
    "\n",
    "**THIS CELL IS A TYPO (copied-pasted without check) - IGNORE IT** No point in adding a coordinate which is identical over all points, if you are doing a tree classifier!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree\n",
    "\n",
    "Now **complete** the class *Tree* and all auxiliary functions. <br>\n",
    "\n",
    "The input parameters to pass to the *id3_training* function are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_depth$: the maximum depth of the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*My notes:*\n",
    "\n",
    "1. *class Tree is actually a NODE object. If the node is internal, it will contain a list of nodes making its left subtree, and a list of nodes making the right subtree (self.left and self.right attributes, respectively). Each node in the subtrees is an object of class Tree by itself.*\n",
    "\n",
    "2. *Entropy function takes as inputs the lists of **labels Y**  (of points in the left subtree and of points in the right subtree). It is a static method, so you call it as Tree.entropy(). \n",
    "\n",
    "4. Careful to always include a `return` in the recursive function `self.classify()`. Also, remember to assign the attributes self.idx and self.thresh to each Tree during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.idx = -1    # The index of the feature over which you split (no split: -1)\n",
    "        self.thresh = 0  # The threshold value over which you split (<=: left, >: right)\n",
    "        self.leaf = 0    # 1 if it is a leaf of class 1, -1 if it is a leaf of class -1, 0 if it is an internal node\n",
    "        self.left = []   # Left subtree (empty if it is a leaf)\n",
    "        self.right = []  # Right subtree (empty if it is a leaf)\n",
    "\n",
    "\n",
    "    def entropy(left, right):\n",
    "        H = 0\n",
    "        tot_length = len(left) + len(right)\n",
    "        left_prob = len(np.where(left > 0)[0]) / len(left)\n",
    "        if (left_prob > 0):\n",
    "            H -= len(left) * left_prob * np.log2(left_prob) / tot_length\n",
    "        if (left_prob < 1):\n",
    "            H -= len(left) * (1 - left_prob) * np.log2(1 - left_prob) / tot_length\n",
    "        right_prob = len(np.where(right > 0)[0]) / len(right)\n",
    "        if (right_prob > 0):\n",
    "            H -= len(right) * right_prob * np.log2(right_prob) / tot_length\n",
    "        if (right_prob < 1):\n",
    "            H -= len(right) * (1 - right_prob) * np.log2(1 - right_prob) / tot_length\n",
    "        return H\n",
    "\n",
    "    def classify(self, x):\n",
    "        ## TODO: classify the point x (easy for leaves, you have to go down the tree if the node is internal)\n",
    "        if (self.leaf != 0):\n",
    "            return self.leaf # return leaf value (+ or - 1) as the point label\n",
    "        elif (x[self.idx] <= self.thresh):\n",
    "            return self.left.classify(x) #  if not a leaf, pass it on to the left or right node TO CHECK AGAIN\n",
    "        else:\n",
    "            return self.right.classify(x)\n",
    "\n",
    "\n",
    "    def id3_training(self, X, Y, max_depth, printing):\n",
    "        # Check if the node is a leaf (all nodes have the same label)\n",
    "        if (np.max(Y) - np.min(Y) < 1e-3):\n",
    "            self.leaf = np.max(Y)\n",
    "            if (printing):\n",
    "                print('Remaining depth: ' + str(max_depth) + ', leaf node (all labels are the same over ' + str(len(Y)) + ' points)')\n",
    "            return\n",
    "        # If the maximum depth is 0, the node must be a leaf!\n",
    "        if (max_depth < 1):\n",
    "            if (printing):\n",
    "                print('Remaining depth: ' + str(max_depth) + ', leaf node (maximum depth reached, ' + str(len(Y)) + ' points)')\n",
    "            if (len(np.where(Y > 0)) > len(Y) / 2):\n",
    "                self.leaf = 1\n",
    "            else:\n",
    "                self.leaf = -1\n",
    "            return\n",
    "        # Find the best split: iterate over features\n",
    "        best_idx = -1\n",
    "        best_thresh = -1\n",
    "        best_entropy = 1e9\n",
    "        ## TODO: Iterate over the features and threshold values! \n",
    "        for idx in range(X.shape[1]):\n",
    "            x_values = np.unique(X[:, idx]) # sorted unique values\n",
    "            thresholds = np.array([(x_values[i+1] + x_values[i])/2 for i in range(len(x_values) - 1)]) # mean values\n",
    "            for t in thresholds:\n",
    "                left_samples =  np.where(X[:, idx] <= t)[0] #returns indexes\n",
    "                right_samples = np.where(X[:, idx] > t)[0]\n",
    "                entropy_value = Tree.entropy(Y[left_samples], Y[right_samples])\n",
    "                if entropy_value <= best_entropy:\n",
    "                    best_idx = idx\n",
    "                    best_thresh = t\n",
    "                    best_entropy = entropy_value\n",
    "        ####-------------------------------------------------------####\n",
    "        if (best_idx == -1):\n",
    "            # No valid features! The points are all identical\n",
    "            #\n",
    "            # my note: entropy always > 10e9 means either that all points have same label (but cannot be,\n",
    "            # because we ruled out that possibility already) or that all points are the same point (so)\n",
    "            # distribution is a dirac delta\n",
    "            self.leaf = np.sign(np.sum(Y))\n",
    "            if (self.leaf == 0):\n",
    "                self.leaf = 1\n",
    "            if (printing):\n",
    "                print('Remaining depth: ' + str(max_depth) + ', leaf node (all inputs are the same over ' + str(len(Y)) + ' points)')\n",
    "            return\n",
    "        left_samples = np.where(X[:, best_idx] <= best_thresh)[0]\n",
    "        right_samples = np.where(X[:, best_idx] > best_thresh)[0]\n",
    "        if (printing):\n",
    "            print('Remaining depth: ' + str(max_depth) + ', splitting ' + str(len(Y)) + ' elements into ' + str(len(left_samples)) + ' and ' + str(len(right_samples)) + ' over feature ' + str(best_idx))\n",
    "        ## TODO: run the next recursive step of ID3 over the left and right subtrees!\n",
    "        # also, we need to set the class attributes self.idx and self.threshold, which are then used for PREDICTION\n",
    "        self.idx = best_idx\n",
    "        self.thresh = best_thresh\n",
    "        self.left = Tree()\n",
    "        self.right = Tree()\n",
    "        self.left.id3_training(X[left_samples], Y[left_samples], max_depth - 1, printing)\n",
    "        self.right.id3_training(X[right_samples], Y[right_samples], max_depth - 1, printing)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extra_training(self, X, Y, max_depth, printing):\n",
    "        # Check if the node is a leaf (all nodes have the same label)\n",
    "        if (np.max(Y) - np.min(Y) < 1e-3):\n",
    "            self.leaf = np.max(Y)\n",
    "            if (printing):\n",
    "                print('Remaining depth: ' + str(max_depth) + ', leaf node (all labels are the same over ' + str(len(Y)) + ' points)')\n",
    "            return\n",
    "        # If the maximum depth is 0, the node must be a leaf!\n",
    "        if (max_depth < 1):\n",
    "            if (printing):\n",
    "                print('Remaining depth: ' + str(max_depth) + ', leaf node (maximum depth reached, ' + str(len(Y)) + ' points)')\n",
    "            if (len(np.where(Y > 0)) > len(Y) / 2):\n",
    "                self.leaf = 1\n",
    "            else:\n",
    "                self.leaf = -1\n",
    "            return\n",
    "        # Find the best split: iterate over features\n",
    "        best_idx = -1\n",
    "        best_thresh = -1\n",
    "        best_entropy = 1e9\n",
    "        ## TODO: Iterate over the features (remember, the threshold value is random)! \n",
    "        for idx in range(X.shape[1]):\n",
    "            x_vals = np.unique(X[:, idx])\n",
    "            thresholds = np.array([(x_vals[i+1] + x_vals[i])/2 for i in range(len(x_vals) - 1)])\n",
    "            if len(thresholds) > 0:\n",
    "                t = np.random.choice(a = thresholds, size = 1)[0]\n",
    "                left_samples =  np.where(X[:, idx] <= t)[0] #returns indexes\n",
    "                right_samples = np.where(X[:, idx] > t)[0]\n",
    "                entropy = Tree.entropy(left_samples, right_samples)\n",
    "                if (entropy <= best_entropy):\n",
    "                    best_idx = idx\n",
    "                    best_entropy = entropy\n",
    "                    best_thresh = t\n",
    "        if (best_idx == -1):\n",
    "            # No valid features! The points are all identical\n",
    "            self.leaf = np.sign(np.sum(Y))\n",
    "            if (self.leaf == 0):\n",
    "                self.leaf = 1\n",
    "            if (printing):\n",
    "                print('Remaining depth: ' + str(max_depth) + ', leaf node (all inputs are the same over ' + str(len(Y)) + ' points)')\n",
    "            return\n",
    "        left_samples = np.where(X[:, best_idx] <= best_thresh)[0]\n",
    "        right_samples = np.where(X[:, best_idx] > best_thresh)[0]\n",
    "        if (printing):\n",
    "            print('Remaining depth: ' + str(max_depth) + ', splitting ' + str(len(Y)) + ' elements into ' + str(len(left_samples)) + ' and ' + str(len(right_samples)) + ' over feature ' + str(best_idx))\n",
    "        ## TODO: run the next recursive step of ID3 over the left and right subtrees!\n",
    "        self.idx = best_idx\n",
    "        self.thresh = best_thresh\n",
    "        self.left = Tree()\n",
    "        self.right = Tree()\n",
    "        self.left.extra_training(X[left_samples], Y[left_samples], max_depth - 1, printing)\n",
    "        self.right.extra_training(X[right_samples], Y[right_samples], max_depth - 1, printing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation to learn a model from the training data directly. Set a maximum depth of 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.19453319133830368\n",
      "Test loss: 0.3255319148936155\n"
     ]
    }
   ],
   "source": [
    "single_tree = Tree()\n",
    "single_tree.id3_training(X_training, Y_training, 12, False)\n",
    "\n",
    "train_loss = 0\n",
    "for i in range(len(Y_training)):\n",
    "    predicted = single_tree.classify(X_training[i, :])\n",
    "    if (Y_training[i] != predicted):\n",
    "        train_loss += 1 / len(Y_training)\n",
    "print('Training loss: ' + str(train_loss))\n",
    "\n",
    "test_loss = 0\n",
    "for i in range(len(Y_test)):\n",
    "    predicted = single_tree.classify(X_test[i, :])\n",
    "    if (Y_test[i] != predicted):\n",
    "        test_loss += 1 / len(Y_test)\n",
    "print('Test loss: ' + str(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This overfits a lot! Let's try to create a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forest:\n",
    "\n",
    "    def __init__(self, trees, n_features, n_samples, max_depth):\n",
    "        self.forest = [] # list of trees\n",
    "        self.features = n_features # my note: to be used in bag()\n",
    "        self.max_depth = max_depth\n",
    "        self.samples = n_samples  # my note: to be used in bag()\n",
    "        for i in range(trees):\n",
    "            self.forest.append(Tree())\n",
    "\n",
    "    def classify(self, x):\n",
    "        # TODO Classify the point through a majority vote\n",
    "        votes = [forest_tree.classify(x) for forest_tree in self.forest]\n",
    "        vote = np.sign(np.sum(votes))\n",
    "        if vote == 0:\n",
    "            vote = +1 # in case of ties, I return 1\n",
    "        return vote\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for tree in self.forest:\n",
    "            X_train, Y_train = self.bag(X, Y)\n",
    "            tree.id3_training(X_train, Y_train, self.max_depth, False)\n",
    "        \n",
    "    def bag(self, X, Y):\n",
    "        ## TODO: implement bagging! Sample data points with replacement\n",
    "        chosen_idxs = np.random.choice(np.arange(X.shape[0]), size = self.samples, replace = True)\n",
    "        if self.features == X.shape[1]:\n",
    "            chosen_features = np.arange(self.features)\n",
    "        else:\n",
    "            chosen_features = np.random.choice(np.arange(X.shape[1]), size = self.features, replace = False)\n",
    "        X_bag = X[chosen_idxs][:, chosen_features]\n",
    "        Y_bag = Y[chosen_idxs]\n",
    "        return X_bag, Y_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a forest with **400** trees, with a maximum depth of 12. We can train each tree on the same number of points as the training set (using **bagging** to add some randomness). Since we do not have that many features, **we keep all 3 features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2861702127659569\n"
     ]
    }
   ],
   "source": [
    "forest = Forest(40, 3, X.shape[0], 12)\n",
    "forest.train(X_training, Y_training)\n",
    "\n",
    "test_loss = 0\n",
    "for i in range(len(Y_test)):\n",
    "    predicted = forest.classify(X_test[i, :])\n",
    "    if (Y_test[i] * predicted <= 0):\n",
    "        test_loss += 1 / len(Y_test)\n",
    "print('Test loss: ' + str(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to see the effect of using Extra Trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtraTrees:\n",
    "\n",
    "    def __init__(self, trees, n_features, n_samples, max_depth):\n",
    "        self.forest = []\n",
    "        self.features = n_features\n",
    "        self.max_depth = max_depth\n",
    "        self.samples = n_samples\n",
    "        for i in range(trees):\n",
    "            self.forest.append(Tree())\n",
    "\n",
    "    def classify(self, x):\n",
    "        vote = 0\n",
    "        for tree in self.forest:\n",
    "            vote += tree.classify(x)\n",
    "        return np.sign(vote)\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for tree in self.forest:\n",
    "            X_train, Y_train = self.bag(X, Y)\n",
    "            tree.extra_training(X_train, Y_train, self.max_depth, False)\n",
    "        \n",
    "    def bag(self, X, Y):\n",
    "        features = X.shape[1]\n",
    "        points = X.shape[0]\n",
    "        # Bagging: sample with replacement\n",
    "        bagged = rnd.choices(range(points), k = self.samples)\n",
    "        X_bagged = X[bagged, :]\n",
    "        # Remove features that are not part of the tree\n",
    "        selected = rnd.sample(range(features), k = self.features)\n",
    "        for i in range(features):\n",
    "            if i not in selected:\n",
    "                X_bagged[:, i] = 0\n",
    "        return X_bagged, Y[bagged]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create an Extra Trees Classifer with 1000 trees, with a maximum depth of 12. We can train each tree on the same number of points as the training set (using bagging to add some randomness). Since we do not have that many features, we keep all 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.25851063829787246\n"
     ]
    }
   ],
   "source": [
    "extra = ExtraTrees(100, 3, X.shape[0], 12)\n",
    "extra.train(X_training, Y_training)\n",
    "\n",
    "test_loss = 0\n",
    "for i in range(len(Y_test)):\n",
    "    predicted = extra.classify(X_test[i, :])\n",
    "    if (Y_test[i] * predicted <= 0):\n",
    "        test_loss += 1 / len(Y_test)\n",
    "print('Test loss: ' + str(test_loss))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
